# IngestÃ£o de Dados Financeiros com Alpha Vantage no Databricks

Este projeto implementa um **pipeline de ingestÃ£o e anÃ¡lise de dados financeiros** utilizando a API **Alpha Vantage** e a plataforma **Databricks**, com armazenamento em **Delta Lake** e governanÃ§a via **Unity Catalog**.

O objetivo Ã© coletar cotaÃ§Ãµes diÃ¡rias de aÃ§Ãµes da B3, armazenar os dados na camada **Bronze** e disponibilizar uma **view analÃ­tica** para consumo posterior.

---

## ğŸ“Œ VisÃ£o Geral do Projeto

O projeto realiza:

1. IngestÃ£o de dados diÃ¡rios de aÃ§Ãµes via **API Alpha Vantage**
2. Tratamento e padronizaÃ§Ã£o dos dados em Python
3. PersistÃªncia em tabela **Delta** (camada Bronze)
4. CriaÃ§Ã£o de **view analÃ­tica** com agregaÃ§Ãµes por ticker

Arquitetura baseada no padrÃ£o **Bronze â†’ Analytics (Gold)**.

---

## ğŸ—‚ Estrutura do RepositÃ³rio

â”œâ”€â”€ api.py # Pipeline de ingestÃ£o da API

â”œâ”€â”€ config.py # ConfiguraÃ§Ãµes do projeto

â”œâ”€â”€ DataView.sql # View analÃ­tica em SQL

â”œâ”€â”€ README.md

---

## âš™ï¸ DescriÃ§Ã£o dos Arquivos

### ğŸ“„ `config.py`
Arquivo responsÃ¡vel pelas **configuraÃ§Ãµes globais do projeto**.

ContÃ©m:
- DefiniÃ§Ã£o do **Unity Catalog**
- Chave da API Alpha Vantage
- URL base da API
- Lista de tickers a serem ingeridos
- Schema e tabela da camada Bronze

Exemplo:
```python
CATALOG = "kpuudata"
TICKERS = ["PETR4.SA", "VALE3.SA", "ITUB4.SA", "BBDC4.SA", "ABEV3.SA"]
```

TambÃ©m cria automaticamente o schema Bronze caso nÃ£o exista.

ğŸ“„ api.py

Notebook/script responsÃ¡vel pela ingestÃ£o dos dados.

Principais etapas:

1- Importa as configuraÃ§Ãµes via:
```
# MAGIC %run "/Workspace/Projeto_Pos_Databricks/injestao_api_databricks/config"
```
2- Consulta a API Alpha Vantage usando a funÃ§Ã£o:
```

buscar_cotacoes_diarias(symbol)
```
3- Trata erros comuns da API:

- Ticker invÃ¡lido

- Limite de requisiÃ§Ãµes

- Respostas inesperadas

4- Padroniza os dados:

- ConversÃ£o de tipos

- RenomeaÃ§Ã£o de colunas

- InclusÃ£o de ticker e data_injestao

5- Consolida todos os tickers em um Ãºnico DataFrame

6- Grava os dados em Delta Lake (append):
```
df_spark.write.format("delta").mode("append").saveAsTable(full_table)
```
ğŸ“Œ ObservaÃ§Ã£o:
Foi adicionado time.sleep(2) entre as chamadas para respeitar o limite da API gratuita.


ğŸ“„ DataView.sql

Script SQL responsÃ¡vel pela camada analÃ­tica.

Funcionalidades:

Define o catÃ¡logo correto

Cria o schema analytics_api

Cria uma view agregada por ticker

View criada:
```sql
analytics_api.vw_cotacoes_resumo
```

AgregaÃ§Ãµes disponÃ­veis:

- Ãšltima data disponÃ­vel

- Maior alta do perÃ­odo

- Menor baixa do perÃ­odo

- PreÃ§o mÃ©dio de fechamento

- Volume total negociado

- Ãšltima data de ingestÃ£o

ğŸ”„ Fluxo do Pipeline

ConfiguraÃ§Ã£o

- DefiniÃ§Ã£o do catÃ¡logo, schema e tickers (config.py)

IngestÃ£o

- Coleta de dados da Alpha Vantage (api.py)

- GravaÃ§Ã£o na tabela Bronze:

kpuudata.alphavantage_bronze.cotacoes_alpha


TransformaÃ§Ã£o AnalÃ­tica

- CriaÃ§Ã£o da view resumida (DataView.sql)

Consumo

- Consultas SQL

- Dashboards

- APIs

- BI

ğŸ§  Tecnologias Utilizadas

Databricks

Apache Spark

Delta Lake

Unity Catalog

Python

SQL

API Alpha Vantage

ğŸ“‹ PrÃ©-requisitos

Workspace Databricks ativo

Cluster Spark em execuÃ§Ã£o

Chave de API da Alpha Vantage
ğŸ‘‰ https://www.alphavantage.co

PermissÃµes para criar schemas e tabelas no Unity Catalog

â–¶ï¸ Como Executar o Projeto

Clone ou importe o repositÃ³rio no Databricks Repos

Atualize sua API_KEY no config.py

Execute o notebook api.py para ingestÃ£o dos dados

Execute o script DataView.sql para criar a view analÃ­tica

Consulte os dados: